Index: high_resolution/attacks/cos_similarity_optimize.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/high_resolution/attacks/cos_similarity_optimize.py b/high_resolution/attacks/cos_similarity_optimize.py
deleted file mode 100644
--- a/high_resolution/attacks/cos_similarity_optimize.py	
+++ /dev/null	
@@ -1,142 +0,0 @@
-from losses.poincare import poincare_loss
-import math
-
-import copy
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-input_features_of_last_layer = None
-
-def forward_hook(module, inputs, output):
-    global input_features_of_last_layer
-    input_features_of_last_layer = inputs[0]
-
-
-def toogle_grad(model, flag=True):
-    for p in model.parameters():
-        p.requires_grad = flag
-
-
-class Optimization():
-    def __init__(self, target_model, synthesis, discriminator, transformations, num_ws, config):
-        # self.synthesis = synthesis
-        self.target = target_model
-        self.discriminator = discriminator
-        self.config = config
-        self.transformations = transformations
-        self.discriminator_weight = self.config.attack['discriminator_loss_weight']
-        self.num_ws = num_ws
-        self.clip = config.attack['clip']
-
-        self.target.module.model.fc.register_forward_hook(forward_hook)
-
-        self.original_synthesis = copy.deepcopy(synthesis)
-
-    def diversity_regularizer(self, feats):
-        pairwise_distance = torch.norm(feats.unsqueeze(1) - feats.unsqueeze(0), p=2, dim=2)
-        diversity_loss = pairwise_distance.mean()
-
-        return diversity_loss
-
-    def optimize(self, w_batch, targets_batch, num_epochs):
-        # Initialize attack
-        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])
-        scheduler = self.config.create_lr_scheduler(optimizer)
-
-        # self.optimizer_w = self.config.selective_create_optimizer(mode="latent_code",
-        #                                                           params=[w_batch.requires_grad_()],
-        #                                                           config=self.config)
-
-        # self.scheduler_w = self.config.create_lr_scheduler(self.optimizer_w)
-        self.synthesis = copy.deepcopy(self.original_synthesis)
-
-        # Start optimization
-        for i in range(num_epochs):
-            # synthesize images and preprocess images
-            imgs = self.synthesize(w_batch, num_ws=self.num_ws)
-
-            # compute discriminator loss
-            if self.discriminator_weight > 0:
-                discriminator_loss = self.compute_discriminator_loss(imgs)
-            else:
-                # discriminator_loss = torch.tensor(0.0)
-                discriminator_loss = self.compute_discriminator_loss(imgs)
-
-            # perform image transformations
-            if self.clip:
-                imgs = self.clip_images(imgs)
-            if self.transformations:
-                imgs = self.transformations(imgs)
-
-            # Compute target loss
-            # print(len(input_features_of_last_layer))  # 0
-            outputs = self.target(imgs)  # [40, 3, 224, 224]
-            # print(len(input_features_of_last_layer))  # 4
-
-            # target_loss = poincare_loss(outputs, targets_batch).mean()
-
-            class_weights = self.target.module.model.fc.weight[targets_batch]
-            input_features = input_features_of_last_layer  # 获取输入特征
-            cosine_similarity = F.cosine_similarity(input_features, class_weights).mean()
-            target_loss = -1 * cosine_similarity
-
-            loss = target_loss + discriminator_loss * self.discriminator_weight
-
-            optimizer.zero_grad()
-            loss.backward()
-            optimizer.step()
-
-            # Log results
-            if self.config.log_progress and ((i + 1) % 10 == 0):
-                with torch.no_grad():
-                    confidence_vector = outputs.softmax(dim=1)
-                    confidences = torch.gather(
-                        confidence_vector, 1, targets_batch.unsqueeze(1))
-                    mean_conf = confidences.mean().detach().cpu()
-
-                    min_conf = confidences.min().detach().cpu().item()  # Get minimum confidence
-                    max_conf = confidences.max().detach().cpu().item()  # Get maximum confidence
-
-                if torch.cuda.current_device() == 0:
-                    print(
-                        f'iteration {i + 1}: \t total_loss={loss:.4f} \t cs_similarity={cosine_similarity.item():.4f} \t',
-                        f'discriminator_loss={discriminator_loss:.4f} \t '
-                        f'mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'
-                    )
-
-        return w_batch.detach()
-
-    def print_synthesis_gradients(self):
-        for name, param in self.synthesis.named_parameters():
-            if param.grad is not None:
-                print(f"Gradients of {name}: \n{param.grad}")
-            else:
-                print(f"No gradients for {name}")
-
-    def synthesize(self, w, num_ws):
-        if w.shape[1] == 1:
-            w_expanded = torch.repeat_interleave(w,
-                                                 repeats=num_ws,
-                                                 dim=1)
-            imgs = self.synthesis(w_expanded,
-                                  noise_mode='const',
-                                  force_fp32=True)
-        else:
-            imgs = self.synthesis(w, noise_mode='const', force_fp32=True)
-        return imgs
-
-    def clip_images(self, imgs):
-        lower_limit = torch.tensor(-1.0).float().to(imgs.device)
-        upper_limit = torch.tensor(1.0).float().to(imgs.device)
-        imgs = torch.where(imgs > upper_limit, upper_limit, imgs)
-        imgs = torch.where(imgs < lower_limit, lower_limit, imgs)
-        return imgs
-
-    def compute_discriminator_loss(self, imgs):
-        discriminator_logits = self.discriminator(imgs, None)
-        discriminator_loss = nn.functional.softplus(
-            -discriminator_logits).mean()
-
-        return discriminator_loss
Index: high_resolution/attacks/optimize.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from losses.poincare import poincare_loss\r\nimport math, os\r\n\r\nimport numpy as np\r\nimport torch, torchvision\r\nimport torch.fft\r\nimport torch.nn as nn\r\nimport PIL\r\nfrom torch.autograd.functional import jacobian\r\nfrom torch.autograd.functional import jvp\r\n\r\nimport torchvision.utils as vutils\r\nimport matplotlib.pyplot as plt\r\nfrom skimage import exposure\r\nfrom matplotlib import colors\r\nfrom matplotlib.colors import TwoSlopeNorm\r\nfrom PIL import Image\r\n\r\nimport torchvision.transforms as T\r\nimport torch.nn.functional as F\r\nfrom torchvision.transforms import InterpolationMode\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n\r\ndef build_transformations(transformations_dict=None) -> T.Compose:\r\n    transformations_dict = {\r\n        # \"RandomResizedCrop\": {\r\n        #     \"size\": 224,\r\n        #     \"scale\": [0.8, 1.0],\r\n        #     \"ratio\": [0.9, 1.1],\r\n        #     \"antialias\": True\r\n        # },\r\n        # \"RandomHorizontalFlip\": {\r\n        #     \"p\": 0.5\r\n        # },\r\n        # \"RandomRotation\": {\r\n        #     \"degrees\": 5,  # ±15°\r\n        #     \"interpolation\": InterpolationMode.BILINEAR,\r\n        #     \"expand\": False,\r\n        #     \"center\": None\r\n        # },\r\n\r\n        \"RandomResizedCrop\": {\r\n            \"size\": 224,\r\n            \"scale\": [0.7, 1.0],\r\n            \"ratio\": [0.8, 1.2],\r\n            \"antialias\": True\r\n        },\r\n        \"RandomHorizontalFlip\": {\r\n            \"p\": 0.5\r\n        },\r\n        \"RandomRotation\": {\r\n            \"degrees\": 5,  # ±15°\r\n            \"interpolation\": InterpolationMode.BILINEAR,\r\n            \"expand\": False,\r\n            \"center\": None\r\n        }\r\n    }\r\n\r\n    transformation_list = []\r\n\r\n    for transform, args in transformations_dict.items():\r\n        if not hasattr(T, transform):\r\n            raise Exception(\r\n                f\"{transform} is not a valid transformation. Please write the type exactly as the Torchvision class.\"\r\n            )\r\n        transformation_class = getattr(T, transform)\r\n        transformation_list.append(transformation_class(**args))\r\n\r\n    if transformation_list:\r\n        return T.Compose(transformation_list)\r\n    else:\r\n        return None\r\n\r\n\r\ndef save_image_grid(img, fname, drange, grid_size):\r\n    lo, hi = drange\r\n    img = np.asarray(img, dtype=np.float32)\r\n    img = (img - lo) * (255 / (hi - lo))\r\n    img = np.rint(img).clip(0, 255).astype(np.uint8)\r\n\r\n    gw, gh = grid_size\r\n    _N, C, H, W = img.shape\r\n    img = img.reshape(gh, gw, C, H, W)\r\n    img = img.transpose(0, 3, 1, 4, 2)\r\n    img = img.reshape(gh * H, gw * W, C)\r\n\r\n    assert C in [1, 3]\r\n    if C == 1:\r\n        PIL.Image.fromarray(img[:, :, 0], 'L').save(fname)\r\n    if C == 3:\r\n        PIL.Image.fromarray(img, 'RGB').save(fname)\r\n\r\n\r\ndef save_grid_with_cmap(tensor, save_path, cmap=\"seismic\", nrow=5, padding=2, handle_imgs=False):\r\n    # RdBu / viridis / seismic\r\n\r\n    # Per-sample max-abs normalization to keep structure while normalizing\r\n    max_vals = tensor.view(tensor.size(0), -1).abs().max(dim=1)[0]  # shape: (B,)\r\n    max_vals = max_vals.view(-1, 1, 1, 1) + 1e-8  # reshape for broadcasting\r\n    tensor = tensor / max_vals  # shape-preserving normalization\r\n\r\n    if handle_imgs:\r\n        grid = vutils.make_grid(tensor, nrow=nrow, normalize=True, scale_each=False, padding=padding, pad_value=0)\r\n        grid_np = grid.cpu().numpy()\r\n        grid_np = np.transpose(grid_np, (1, 2, 0))\r\n        plt.imsave(save_path, grid_np)\r\n    else:\r\n        grid = vutils.make_grid(tensor, nrow=nrow, normalize=False, scale_each=False, padding=padding, pad_value=-1.0)\r\n        grid_np = grid.cpu().numpy()\r\n        if grid_np.shape[0] == 1:\r\n            grid_np = grid_np[0]  # (H, W)\r\n        else:\r\n            grid_np = np.mean(grid_np, axis=0)  # (H, W)\r\n\r\n        # Display and save\r\n        plt.figure(figsize=(10, 10))\r\n\r\n        # 1) Alternative: plt.imshow(grid_np, cmap=cmap, norm=colors.CenteredNorm())\r\n        # 2) Use symmetric norm around zero\r\n        vmax = np.max(np.abs(grid_np))\r\n        # norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\r\n        norm = colors.CenteredNorm()\r\n        plt.imshow(grid_np, cmap=cmap, norm=norm)\r\n\r\n        plt.axis(\"off\")\r\n        plt.savefig(save_path, bbox_inches=\"tight\", pad_inches=0, dpi=300)\r\n        plt.close()\r\n\r\n\r\ndef cosine_similarity_batch(a: torch.Tensor, b: torch.Tensor):\r\n    \"\"\"\r\n    Compute per-sample cosine similarity between two tensors of shape (B, C, H, W)\r\n    \"\"\"\r\n    a = a.view(a.size(0), -1)  # (B, D)\r\n    b = b.view(b.size(0), -1)  # (B, D)\r\n\r\n    a_norm = a / (a.norm(dim=1, keepdim=True) + 1e-8)\r\n    b_norm = b / (b.norm(dim=1, keepdim=True) + 1e-8)\r\n\r\n    return (a_norm * b_norm).sum(dim=1)  # shape (B,)\r\n\r\n\r\n# Visualize dL/dx\r\nclass Optimization():\r\n    def __init__(self, target_model, synthesis, discriminator, transformations, num_ws, config, save_dir=None):\r\n        self.synthesis = synthesis\r\n        self.target = target_model\r\n        self.discriminator = discriminator\r\n        self.config = config\r\n        self.transformations = transformations\r\n        self.discriminator_weight = self.config.attack['discriminator_loss_weight']\r\n        self.num_ws = num_ws\r\n        self.clip = config.attack['clip']\r\n        self.save_dir = save_dir\r\n\r\n        self.log_progress = 1\r\n        self.visualize_intermediate_results = 1\r\n\r\n    def G_forward(self, w):\r\n        return self.synthesize(w, num_ws=self.num_ws)\r\n\r\n    def optimize(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        self.visualize_intermediate_results = 1\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(\r\n                outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            imgs.retain_grad()\r\n            loss.backward()  # backward once\r\n\r\n            grad_x = imgs.grad.detach().cpu()\r\n\r\n            vis_condition = self.save_dir and ((i + 1) % 20 == 0 or i == 0 or (i + 1) == 70)\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            self.visualize_intermediate_results = 0\r\n            if self.visualize_intermediate_results:\r\n                if vis_condition:\r\n                    torchvision.utils.save_image(\r\n                        torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                        os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png')\r\n                    )\r\n\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    save_grid_with_cmap(\r\n                        proj_x, os.path.join(self.save_dir,\r\n                                             f'proj_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    del grad_w, grad_x, proj_x\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def optimize_demo(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        self.visualize_intermediate_results = 0\r\n\r\n        # List to store (epoch_num, norm_ratio) pairs\r\n        epoch_records = []\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(\r\n                outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            imgs.retain_grad()\r\n            loss.backward()  # backward once\r\n\r\n            grad_x = imgs.grad.detach()\r\n\r\n            vis_condition = self.save_dir and ((i + 1) % 20 == 0 or i == 0 or (i + 1) == 70)\r\n            # if self.visualize_intermediate_results and vis_condition:\r\n            if (i + 1) % 10 == 0:\r\n                #\r\n                grad_w = w_batch.grad.detach()\r\n                proj_x = self.compute_proj_x_orthogonalized(imgs, w_batch, grad_x, chunk_size=50)\r\n                proj_x = proj_x.detach()\r\n\r\n                B = grad_x.shape[0]\r\n\r\n                norm_grad = torch.norm(grad_x.view(B, -1), dim=1)\r\n                norm_proj = torch.norm(proj_x.view(B, -1), dim=1)\r\n                norm_ratio = norm_proj / norm_grad\r\n\r\n                mean_ratio = norm_ratio.mean().item()\r\n                min_ratio = norm_ratio.min().item()\r\n                max_ratio = norm_ratio.max().item()\r\n\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n\r\n                for idx in range(B):\r\n                    epoch_records.append({\r\n                        'epoch': i + 1,\r\n                        'cosine': norm_ratio[idx].item(),\r\n                        'conf': confidences[idx].item(),\r\n                    })\r\n\r\n                print(f\"[Norm Ratio] mean: {mean_ratio:.4f}, min: {min_ratio:.4f}, max: {max_ratio:.4f}\")\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                if vis_condition:\r\n                    torchvision.utils.save_image(\r\n                        torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                        os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png')\r\n                    )\r\n\r\n                    grad_x = imgs.grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    proj_x = proj_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        proj_x, os.path.join(self.save_dir,\r\n                                             f'proj_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    # save_grid_with_cmap(\r\n                    #     residual, os.path.join(self.save_dir, f'res_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                    #     nrow=5\r\n                    # )\r\n\r\n                    del grad_w, grad_x, proj_x\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach(), epoch_records\r\n\r\n    def optimization_PAA(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        num_samples = 50  # SmoothGrad sampling times\r\n        stdev_spread = 0.06\r\n\r\n        epoch_records = []\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n            sigma = stdev_spread * (imgs.max().item() - imgs.min().item())\r\n\r\n            # Use SmoothGrad to compute smoothed gradients:\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                noisy_imgs = imgs + torch.randn_like(imgs) * sigma\r\n                outputs_noise = self.target(noisy_imgs)\r\n                loss_noise = poincare_loss(outputs_noise, targets_batch).mean()\r\n\r\n                grad_noise = torch.autograd.grad(loss_noise, noisy_imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_noise\r\n\r\n            smooth_grad /= num_samples\r\n\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=smooth_grad, retain_graph=False)[0]\r\n            w_batch.grad = w_grad\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            self.visualize_intermediate_results = 0\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        noisy_imgs.detach().cpu(),\r\n                        os.path.join(self.save_dir, f'x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5, handle_imgs=1\r\n                    )\r\n\r\n                    grad_x = grad_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    smooth_grad_x = smooth_grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def optimization_TAA(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        # original parameter\r\n        num_samples = 50\r\n\r\n        transformations = build_transformations()\r\n        epoch_records = []\r\n\r\n        for i in range(num_epochs):\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n            grad_x = grad_x.detach()\r\n\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                trans_imgs = transformations(imgs)\r\n\r\n                outputs_trans = self.target(trans_imgs)\r\n                loss_trans = poincare_loss(outputs_trans, targets_batch).mean()\r\n\r\n                grad_trans = torch.autograd.grad(loss_trans, imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_trans\r\n\r\n            smooth_grad /= num_samples\r\n            smooth_grad_x = smooth_grad.detach().cpu()\r\n\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=smooth_grad, retain_graph=False)[0]\r\n            w_batch.grad = w_grad\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            self.visualize_intermediate_results = 0\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        trans_imgs.detach().cpu(),\r\n                        os.path.join(self.save_dir, f'x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5, handle_imgs=1\r\n                    )\r\n                    grad_x = grad_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x.detach().cpu(), os.path.join(self.save_dir,\r\n                                                            f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    smooth_grad_x = smooth_grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def optimization_TAA_jvp(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        num_samples = 50  # SmoothGrad sampling times\r\n        transformations = build_transformations()\r\n        epoch_records = []\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                trans_imgs = transformations(imgs)\r\n\r\n                outputs_trans = self.target(trans_imgs)\r\n                loss_trans = poincare_loss(outputs_trans, targets_batch).mean()\r\n\r\n                grad_trans = torch.autograd.grad(loss_trans, imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_trans\r\n\r\n            smooth_grad /= num_samples\r\n            smooth_grad_x = smooth_grad.detach().cpu()\r\n\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=smooth_grad, retain_graph=False)[0]\r\n            w_batch.grad = w_grad\r\n\r\n            if (i + 1) % 10 == 0:\r\n                #\r\n                grad_w = w_batch.grad.detach()\r\n                proj_x = self.compute_proj_x_orthogonalized(imgs, w_batch, smooth_grad, chunk_size=30)\r\n                proj_x = proj_x.detach()\r\n\r\n                B = grad_x.shape[0]\r\n\r\n                norm_grad = torch.norm(smooth_grad.view(B, -1), dim=1)\r\n                norm_proj = torch.norm(proj_x.view(B, -1), dim=1)\r\n                norm_ratio = norm_proj / norm_grad\r\n\r\n                mean_ratio = norm_ratio.mean().item()\r\n                min_ratio = norm_ratio.min().item()\r\n                max_ratio = norm_ratio.max().item()\r\n\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n\r\n                print(f\"[Norm Ratio] mean: {mean_ratio:.4f}, min: {min_ratio:.4f}, max: {max_ratio:.4f}\")\r\n\r\n                for idx in range(B):\r\n                    epoch_records.append({\r\n                        'epoch': i + 1,\r\n                        'cosine': norm_ratio[idx].item(),\r\n                        'conf': confidences[idx].item(),\r\n                    })\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            self.visualize_intermediate_results = 0\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        noisy_imgs.detach().cpu(),\r\n                        os.path.join(self.save_dir, f'x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5, handle_imgs=1\r\n                    )\r\n\r\n                    grad_x = grad_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    smooth_grad_x = smooth_grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach(), epoch_records\r\n\r\n    def optimization_PAA_jvp(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        num_samples = 50  # SmoothGrad sampling times\r\n        stdev_spread = 0.1\r\n\r\n        epoch_records = []\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n            sigma = stdev_spread * (imgs.max().item() - imgs.min().item())\r\n\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                noisy_imgs = imgs + torch.randn_like(imgs) * sigma\r\n                outputs_noise = self.target(noisy_imgs)\r\n                loss_noise = poincare_loss(outputs_noise, targets_batch).mean()\r\n\r\n                grad_noise = torch.autograd.grad(loss_noise, noisy_imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_noise\r\n\r\n            smooth_grad /= num_samples\r\n\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=smooth_grad, retain_graph=False)[0]\r\n            w_batch.grad = w_grad\r\n\r\n            if (i + 1) % 10 == 0:\r\n                #\r\n                grad_w = w_batch.grad.detach()\r\n                proj_x = self.compute_proj_x_orthogonalized(imgs, w_batch, smooth_grad, chunk_size=30)\r\n                proj_x = proj_x.detach()\r\n\r\n                B = grad_x.shape[0]\r\n\r\n                norm_grad = torch.norm(smooth_grad.view(B, -1), dim=1)\r\n                norm_proj = torch.norm(proj_x.view(B, -1), dim=1)\r\n                norm_ratio = norm_proj / norm_grad\r\n\r\n                mean_ratio = norm_ratio.mean().item()\r\n                min_ratio = norm_ratio.min().item()\r\n                max_ratio = norm_ratio.max().item()\r\n\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n\r\n                print(f\"[Norm Ratio] mean: {mean_ratio:.4f}, min: {min_ratio:.4f}, max: {max_ratio:.4f}\")\r\n\r\n                for idx in range(B):\r\n                    epoch_records.append({\r\n                        'epoch': i + 1,\r\n                        'cosine': norm_ratio[idx].item(),\r\n                        'conf': confidences[idx].item(),\r\n                    })\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            self.visualize_intermediate_results = 0\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        noisy_imgs.detach().cpu(),\r\n                        os.path.join(self.save_dir, f'x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5, handle_imgs=1\r\n                    )\r\n\r\n                    grad_x = grad_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    smooth_grad_x = smooth_grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach(), epoch_records\r\n\r\n    def optimize_(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize optimizer and scheduler\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        for i in range(num_epochs):\r\n            # Synthesize images and preprocess\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target outputs and loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            optimizer.zero_grad()\r\n            # Backward to update parameters\r\n            loss.backward(retain_graph=True)\r\n            optimizer.step()\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # --- compute gradients: dL/dx and dfi/dx ---\r\n            grad_L = torch.autograd.grad(loss, imgs, retain_graph=True, allow_unused=True)[0]\r\n            # fi = outputs.gather(1, targets_batch.unsqueeze(1)).squeeze(1)\r\n            # grad_fi = torch.autograd.grad(fi, imgs, retain_graph=True, allow_unused=True)[0]\r\n\r\n            fi = outputs.gather(1, targets_batch.unsqueeze(1)).squeeze(1)\r\n            grad_fi = \\\r\n                torch.autograd.grad(fi, imgs, grad_outputs=torch.ones_like(fi), retain_graph=True, allow_unused=True)[0]\r\n\r\n            # compute dL/dx related projection\r\n            # Already obtained w_batch.grad via loss.backward()\r\n            grad_w = w_batch.grad.detach()\r\n            _, proj_grad_L = jvp(self.G_forward, (w_batch,), (grad_w,))\r\n            proj_grad_L = proj_grad_L.detach().cpu()\r\n\r\n            # Compute projection related to dfi/dx\r\n            # Here assume f_i(x) is the target-class logit in the model output\r\n            # Compute gradient of f_i w.r.t. latent code\r\n            grad_w_fi = torch.autograd.grad(\r\n                fi, w_batch, grad_outputs=torch.ones_like(fi), retain_graph=True, allow_unused=True\r\n            )[0]\r\n            _, proj_grad_fi = jvp(self.G_forward, (w_batch,), (grad_w_fi,))\r\n            proj_grad_fi = proj_grad_fi.detach().cpu()\r\n\r\n            if self.transformations:\r\n                proj_grad_L = self.transformations(proj_grad_L)\r\n                proj_grad_fi = self.transformations(proj_grad_fi)\r\n\r\n            # Move to CPU for visualization\r\n            grad_L_vis = grad_L.detach().cpu()\r\n            grad_fi_vis = grad_fi.detach().cpu()\r\n            proj_grad_L_vis = proj_grad_L.detach().cpu()\r\n            proj_grad_fi_vis = proj_grad_fi.detach().cpu()\r\n\r\n            # Compare alignment via cosine similarity after normalization\r\n            cos_sim_L = cosine_similarity_batch(grad_L_vis, proj_grad_L_vis)  # shape: (B,)\r\n            cos_sim_fi = cosine_similarity_batch(grad_fi_vis, proj_grad_fi_vis)\r\n            mean_sim_L = cos_sim_L.mean().item()\r\n            mean_sim_fi = cos_sim_fi.mean().item()\r\n            print(f\"[Cosine Similarity dL/dx] mean: {mean_sim_L:.4f}\")\r\n            print(f\"[Cosine Similarity dfi/dx] mean: {mean_sim_fi:.4f}\")\r\n\r\n            # --- Visualization ---\r\n            if self.visualize_intermediate_results:\r\n                torchvision.utils.save_image(\r\n                    torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                    os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_imgs.png')\r\n                )\r\n                save_grid_with_cmap(\r\n                    grad_L_vis, os.path.join(self.save_dir, f'gradL_Batch_{batch_i + 1}_iter_{i + 1:02d}.png'),\r\n                    nrow=5\r\n                )\r\n                save_grid_with_cmap(\r\n                    grad_fi_vis, os.path.join(self.save_dir, f'gradfi_Batch_{batch_i + 1}_iter_{i + 1:02d}.png'),\r\n                    nrow=5\r\n                )\r\n                save_grid_with_cmap(\r\n                    proj_grad_L_vis,\r\n                    os.path.join(self.save_dir, f'proj_gradL_Batch_{batch_i + 1}_iter_{i + 1:02d}.png'),\r\n                    nrow=5\r\n                )\r\n                save_grid_with_cmap(\r\n                    proj_grad_fi_vis,\r\n                    os.path.join(self.save_dir, f'proj_gradfi_Batch_{batch_i + 1}_iter_{i + 1:02d}.png'),\r\n                    nrow=5\r\n                )\r\n\r\n            if self.log_progress:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def synthesize(self, w, num_ws):\r\n        if w.shape[1] == 1:\r\n            w_expanded = torch.repeat_interleave(w,\r\n                                                 repeats=num_ws,\r\n                                                 dim=1)\r\n            imgs = self.synthesis(w_expanded,\r\n                                  noise_mode='const',\r\n                                  force_fp32=True)\r\n        else:\r\n            imgs = self.synthesis(w, noise_mode='const', force_fp32=True)\r\n        return imgs\r\n\r\n    def clip_images(self, imgs):\r\n        lower_limit = torch.tensor(-1.0).float().to(imgs.device)\r\n        upper_limit = torch.tensor(1.0).float().to(imgs.device)\r\n        imgs = torch.where(imgs > upper_limit, upper_limit, imgs)\r\n        imgs = torch.where(imgs < lower_limit, lower_limit, imgs)\r\n        return imgs\r\n\r\n    def compute_discriminator_loss(self, imgs):\r\n        print(imgs.shape)\r\n        discriminator_logits = self.discriminator(imgs, None)\r\n        discriminator_loss = nn.functional.softplus(-discriminator_logits).mean()\r\n        return discriminator_loss\r\n\r\n    def compute_proj_x_orthogonalized(self, imgs, w_batch, grad, chunk_size=100):\r\n        proj_x_list = []\r\n        B, _, latent_dim = w_batch.shape\r\n        eye = torch.eye(latent_dim, device=w_batch.device)\r\n\r\n        def G_forward(w):\r\n            gen = self.synthesis.module if hasattr(self.synthesis, 'module') else self.synthesis\r\n            return gen(w)\r\n\r\n        for i in range(B):\r\n            # Fetch the i-th sample's w and the corresponding grad\r\n            w_i = w_batch[i].detach().requires_grad_()  # [1, 1, latent_dim]\r\n            w_i = w_i.unsqueeze(0).repeat(1, self.num_ws, 1)\r\n\r\n            output_shape = grad[i].shape\r\n\r\n            # Define a function that flattens the generator output\r\n            def flattened_g(_w):\r\n                out = G_forward(_w)\r\n\r\n                if self.transformations:\r\n                    out = self.transformations(out)\r\n\r\n                return out.flatten(start_dim=1)\r\n\r\n            v_chunks = torch.split(eye, chunk_size, dim=0)  # each chunk shape: [chunk_size, z_dim]\r\n            tangent_chunks = []\r\n\r\n            for v_chunk in v_chunks:\r\n                v_chunk_expanded = v_chunk.unsqueeze(1).expand(-1, self.num_ws, -1)  # [chunk_size, self.num_ws, z_dim]\r\n                w_i_exp = w_i.expand(v_chunk_expanded.shape[0], *w_i.shape[1:])\r\n\r\n                _, tangent_chunk = jvp(flattened_g, (w_i_exp,), (v_chunk_expanded,))\r\n                tangent_chunks.append(tangent_chunk)\r\n\r\n            tangent_all = torch.cat(tangent_chunks, dim=0)\r\n            J = tangent_all.transpose(0, 1)\r\n\r\n            # J = J.view(grad[i].numel(), -1)\r\n            J = J.contiguous().view(grad[i].numel(), -1)\r\n\r\n            # SVD to get an orthonormal basis U\r\n            U, _, _ = torch.linalg.svd(J, full_matrices=False)\r\n\r\n            # u_check = U.T @ U\r\n            # I = torch.eye(u_check.shape[0], device=u_check.device)\r\n            # orthonorm_error = torch.norm(u_check - I)\r\n            # print(f\"[compute_proj_x_orthogonalized] Sample {i}, orthonorm error = {orthonorm_error.item():.6f}\")\r\n\r\n            # Extract the real image gradient (assume imgs.grad exists, shape [B, C, H, W])\r\n            # real_grad = imgs.grad[i].view(-1)  # [output_dim]\r\n            real_grad = grad[i].view(-1)\r\n\r\n            # Project the real gradient onto the column space of U\r\n            proj_x = U @ (U.T @ real_grad)\r\n            proj_x = proj_x.view(output_shape)\r\n            proj_x_list.append(proj_x.detach())\r\n\r\n        return torch.stack(proj_x_list)\r\n\r\n    def compute_proj_and_basis(self, imgs, w_batch, grad, chunk_size=100):\r\n        \"\"\"\r\n        imgs:    [B, C, H, W]  (real images, only used to obtain shape)\r\n        w_batch: [B, 1, d_latent]\r\n        grad:    [B, C, H, W]  (precomputed real gradients)\r\n        \"\"\"\r\n        proj_x_list = []\r\n        U_list = []\r\n\r\n        B, C, H, W = imgs.shape\r\n        D_out = C * H * W\r\n        _, _, d_latent = w_batch.shape\r\n        eye = torch.eye(d_latent, device=w_batch.device)\r\n\r\n        # Used to call synthesis network\r\n        def G_forward(w):\r\n            gen = self.synthesis.module if hasattr(self.synthesis, 'module') else self.synthesis\r\n            return gen(w)\r\n\r\n        for i in range(B):\r\n            # 1) Prepare single-sample latent input\r\n            w_i = w_batch[i].detach().requires_grad_()  # [1, 1, d_latent]\r\n            w_i = w_i.unsqueeze(0).repeat(1, self.num_ws, 1)  # [1, num_ws, d_latent]\r\n\r\n            # 2) Define flattened generator output\r\n            def flattened_g(_w):\r\n                out = G_forward(_w)  # e.g. [n, C, H, W]\r\n                if self.transformations:\r\n                    out = self.transformations(out)\r\n                return out.flatten(start_dim=1)  # [n, D_out]\r\n\r\n            # 3) Compute JVP in chunks, concatenate into Jacobian J: [D_out, d_latent]\r\n            J_chunks = []\r\n            for v_chunk in torch.split(eye, chunk_size, dim=0):  # [k, d_latent]\r\n                # Expand to match w_i batch shape\r\n                w_rep = w_i.expand(v_chunk.size(0), *w_i.shape[1:])  # [k, num_ws, d_latent]\r\n                v_rep = v_chunk.view(v_chunk.size(0), 1, d_latent)  # [k,1,d_latent]\r\n                v_rep = v_rep.expand(-1, self.num_ws, -1)  # [k,num_ws,d_latent]\r\n\r\n                _, Jc = jvp(flattened_g, (w_rep,), (v_rep,))  # Jc: [k, D_out]\r\n                J_chunks.append(Jc)\r\n\r\n            J = torch.cat(J_chunks, dim=0)  # [d_latent, D_out] if chunk covers all latent\r\n            # Transpose to [D_out, d_latent]\r\n            J = J.transpose(0, 1).contiguous()\r\n\r\n            # 4) SVD → U basis (orthonormal basis vectors in image space)\r\n            #    J = U S V^T, U.shape = [D_out, d_latent]\r\n            U, S, Vt = torch.linalg.svd(J, full_matrices=False)\r\n\r\n            U_list.append(U.detach())\r\n\r\n        U_tensor = torch.stack(U_list, dim=0)  # [B, D_out, d_latent]\r\n        return U_tensor\r\n\r\n    def smooth_grad_optimization_jvp(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        self.visualize_intermediate_results = 1\r\n\r\n        num_samples = 100  # SmoothGrad sampling count\r\n        stdev_spread = 0.1\r\n\r\n        epoch_records = []\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n            sigma = stdev_spread * (imgs.max().item() - imgs.min().item())\r\n\r\n            # Use SmoothGrad for smoothed gradient computation:\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                noisy_imgs = imgs + torch.randn_like(imgs) * sigma\r\n                outputs_noise = self.target(noisy_imgs)\r\n                loss_noise = poincare_loss(outputs_noise, targets_batch).mean()\r\n\r\n                grad_noise = torch.autograd.grad(loss_noise, noisy_imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_noise\r\n\r\n            smooth_grad /= num_samples\r\n\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=smooth_grad, retain_graph=False)[0]\r\n            w_batch.grad = w_grad\r\n\r\n            if (i + 1) % 10 == 0:\r\n                #\r\n                grad_w = w_batch.grad.detach()\r\n                proj_x = self.compute_proj_x_orthogonalized(imgs, w_batch, smooth_grad, chunk_size=60)\r\n                proj_x = proj_x.detach()\r\n\r\n                B = grad_x.shape[0]\r\n\r\n                # Compute L2 norms after flattening per sample\r\n                norm_grad = torch.norm(smooth_grad.view(B, -1), dim=1)\r\n                norm_proj = torch.norm(proj_x.view(B, -1), dim=1)\r\n                norm_ratio = norm_proj / norm_grad  # ratio per sample\r\n\r\n                mean_ratio = norm_ratio.mean().item()\r\n                min_ratio = norm_ratio.min().item()\r\n                max_ratio = norm_ratio.max().item()\r\n\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n\r\n                print(f\"[Norm Ratio] mean: {mean_ratio:.4f}, min: {min_ratio:.4f}, max: {max_ratio:.4f}\")\r\n\r\n                for idx in range(B):\r\n                    epoch_records.append({\r\n                        'epoch': i + 1,\r\n                        'cosine': norm_ratio[idx].item(),\r\n                        'conf': confidences[idx].item(),\r\n                    })\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            self.visualize_intermediate_results = 0\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        noisy_imgs.detach().cpu(),\r\n                        os.path.join(self.save_dir, f'x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5, handle_imgs=1\r\n                    )\r\n\r\n                    grad_x = grad_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    smooth_grad_x = smooth_grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def smooth_manifold_grad_optimization(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        \"\"\"\r\n        Manifold-aware smoothing based optimization (manifold tangent-space smoothing).\r\n        \"\"\"\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        # Smoothing parameters\r\n        num_samples = 100  # number of smoothing samples\r\n        stdev_spread = 0.5  # noise scale relative to the latent-space range\r\n\r\n        w_mean = w_batch.mean(dim=0)  # [w_dim]\r\n        w_std = w_batch.std(dim=0)  # [w_dim]\r\n        low_p, high_p = 0.5, 99.5\r\n        q_low = low_p / 100.0\r\n        q_high = high_p / 100.0\r\n        w_low = torch.quantile(w_batch, q_low, dim=0)  # [w_dim]\r\n        w_high = torch.quantile(w_batch, q_high, dim=0)\r\n        # Compute clip_range\r\n        d_low = (w_mean - w_low).abs().max().item()\r\n        d_high = (w_high - w_mean).abs().max().item()\r\n        clip_range = max(d_low, d_high)\r\n        print(f\"Recommend clip_range = {clip_range:.3f}\")\r\n\r\n        clip_range = 2.124\r\n        sigma_z = stdev_spread * clip_range\r\n\r\n        epoch_records = []\r\n\r\n        for i in range(num_epochs):\r\n            # 1) Forward synthesis\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)  # [B, C, H, W]\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0, device=imgs.device)\r\n\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n            B, C, H, W = imgs.shape\r\n\r\n            optimizer.zero_grad()\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n            loss = target_loss  # + discriminator_weight * ...\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=False)[0]  # [B, C, H, W]\r\n\r\n            U_tensor = self.compute_proj_and_basis(imgs, w_batch, grad_x, chunk_size=50)\r\n            D_out = C * H * W\r\n\r\n            smooth_grad = torch.zeros_like(grad_x)\r\n            for _ in range(num_samples):\r\n                # 4.1) Sample noise in latent subspace [B, d_latent]\r\n                eps_z = torch.randn(B, U_tensor.size(-1), device=imgs.device) * sigma_z\r\n                # 4.2) Map back to image space: eps_x_flat[b] = U[b] @ eps_z[b]\r\n                eps_x_flat = torch.bmm(U_tensor, eps_z.unsqueeze(-1)).squeeze(-1)  # [B, D_out]\r\n                eps_x = eps_x_flat.view(B, C, H, W)\r\n                # 4.3) Form noisy images and compute gradients\r\n                noisy_imgs = imgs + eps_x\r\n                outs_noise = self.target(noisy_imgs)\r\n                loss_noise = poincare_loss(outs_noise, targets_batch).mean()\r\n                grad_noise = torch.autograd.grad(loss_noise, noisy_imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_noise\r\n\r\n            smooth_grad /= num_samples  # smoothed gradients [B, C, H, W]\r\n\r\n            # 5) Backpropagate smoothed gradients to latent space\r\n            #    dL/dw = ∂x/∂w^T @ smooth_grad_flat\r\n            w_grad = torch.autograd.grad(\r\n                outputs=imgs, inputs=w_batch,\r\n                grad_outputs=smooth_grad,\r\n                retain_graph=False\r\n            )[0]\r\n            w_batch.grad = w_grad\r\n\r\n            if i == 0 or (i + 1) % 10 == 0:\r\n                grad_w = w_batch.grad.detach()\r\n\r\n                proj_x_list = []\r\n                B = smooth_grad.shape[0]\r\n                for i in range(B):\r\n                    real_grad = smooth_grad[i].view(-1)\r\n                    U = U_tensor[i]\r\n\r\n                    # Project the real gradient onto the column space of U\r\n                    proj_x = U @ (U.T @ real_grad)\r\n                    proj_x = proj_x.view(smooth_grad[i].shape)\r\n                    proj_x_list.append(proj_x.detach())\r\n\r\n                proj_x = torch.stack(proj_x_list)\r\n\r\n                # Compute L2 norms per sample after flattening\r\n                norm_grad = torch.norm(smooth_grad.view(B, -1), dim=1)\r\n                norm_proj = torch.norm(proj_x.view(B, -1), dim=1)\r\n                norm_ratio = norm_proj / norm_grad  # ratio per sample\r\n\r\n                mean_ratio = norm_ratio.mean().item()\r\n                min_ratio = norm_ratio.min().item()\r\n                max_ratio = norm_ratio.max().item()\r\n\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n\r\n                print(f\"[Norm Ratio] mean: {mean_ratio:.4f}, min: {min_ratio:.4f}, max: {max_ratio:.4f}\")\r\n\r\n                for idx in range(B):\r\n                    epoch_records.append({\r\n                        'epoch': i + 1,\r\n                        'cosine': norm_ratio[idx].item(),\r\n                        'conf': confidences[idx].item(),\r\n                    })\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        noisy_imgs.detach().cpu(),\r\n                        os.path.join(self.save_dir, f'x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5, handle_imgs=1\r\n                    )\r\n\r\n                    grad_x = grad_x.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    smooth_grad_x = smooth_grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_conf_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n\r\n# Test SmoothGrad\r\nclass Optimization_():\r\n    def __init__(self, target_model, synthesis, discriminator, transformations, num_ws, config, save_dir=None):\r\n        self.synthesis = synthesis\r\n        self.target = target_model\r\n        self.discriminator = discriminator\r\n        self.config = config\r\n        self.transformations = transformations\r\n        self.discriminator_weight = self.config.attack['discriminator_loss_weight']\r\n        self.num_ws = num_ws\r\n        self.clip = config.attack['clip']\r\n        self.save_dir = save_dir\r\n        self.log_progress = 1\r\n        self.visualize_intermediate_results = 1\r\n\r\n    def confidence_gain(self, batch_i, w_batch, targets_batch, total_epochs, inner_epochs=10):\r\n        evaluation_model = self.config.create_evaluation_model()\r\n        evaluation_model = torch.nn.DataParallel(evaluation_model)\r\n        evaluation_model.to(device)\r\n        evaluation_model.eval()\r\n\r\n        self.adv_model = self.config.create_adv_trained_model().to(device)\r\n\r\n        from torchvision import transforms\r\n        self.transformations = transforms.Compose([\r\n            transforms.CenterCrop(size=(800, 800)),\r\n            transforms.Resize(size=299, interpolation=transforms.InterpolationMode.BILINEAR, max_size=None,\r\n                              antialias=True)\r\n        ])\r\n\r\n        self.log_progress = False\r\n        self.visualize_intermediate_results = 0\r\n\r\n        gains_regular = []\r\n        gains_smooth = []\r\n\r\n        for epoch in range(0, total_epochs, inner_epochs):\r\n            with torch.no_grad():\r\n                imgs_pre = self.synthesize(w_batch, num_ws=self.num_ws)\r\n                if self.clip:\r\n                    imgs_pre = self.clip_images(imgs_pre)\r\n                if self.transformations:\r\n                    imgs_pre = self.transformations(imgs_pre)\r\n                outputs_pre = evaluation_model(imgs_pre)\r\n                confidence_vector_pre = outputs_pre.softmax(dim=1)\r\n                conf_pre = torch.gather(confidence_vector_pre, 1, targets_batch.unsqueeze(1)).mean()\r\n\r\n            w_batch_smooth = w_batch.detach().clone()\r\n            w_batch = self.optimize(batch_i, w_batch, targets_batch, inner_epochs)\r\n            w_batch_smooth = self.smooth_grad_optimization_with_transforms(batch_i, w_batch_smooth, targets_batch,\r\n                                                                           inner_epochs)\r\n\r\n            with torch.no_grad():\r\n                imgs_regular = self.synthesize(w_batch, num_ws=self.num_ws)\r\n                if self.clip:\r\n                    imgs_regular = self.clip_images(imgs_regular)\r\n                if self.transformations:\r\n                    imgs_regular = self.transformations(imgs_regular)\r\n                outputs_regular = evaluation_model(imgs_regular)\r\n                conf_vec_regular = outputs_regular.softmax(dim=1)\r\n                conf_regular = torch.gather(conf_vec_regular, 1, targets_batch.unsqueeze(1)).mean()\r\n\r\n                imgs_smooth = self.synthesize(w_batch_smooth, num_ws=self.num_ws)\r\n                if self.clip:\r\n                    imgs_smooth = self.clip_images(imgs_smooth)\r\n                if self.transformations:\r\n                    imgs_smooth = self.transformations(imgs_smooth)\r\n                outputs_smooth = evaluation_model(imgs_smooth)\r\n                conf_vec_smooth = outputs_smooth.softmax(dim=1)\r\n                conf_smooth = torch.gather(conf_vec_smooth, 1, targets_batch.unsqueeze(1)).mean()\r\n\r\n            gain_regular = conf_regular - conf_pre\r\n            gain_smooth = conf_smooth - conf_pre\r\n            gains_regular.append(gain_regular.item())\r\n            gains_smooth.append(gain_smooth.item())\r\n\r\n            print(f\"Batch_{batch_i + 1}, [Epoch {epoch + inner_epochs}/{total_epochs}] \"\r\n                  f\"Confidence Gain (regular) = {gain_regular:.4f}, \"\r\n                  f\"Confidence Gain (smooth) = {gain_smooth:.4f}\")\r\n\r\n        return\r\n\r\n    def optimize_autograd(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize images and preprocess\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = self.compute_discriminator_logit(imgs)\r\n\r\n            # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n            loss = target_loss + self.discriminator_weight * discriminator_loss\r\n\r\n            # Zero gradients\r\n            optimizer.zero_grad()\r\n\r\n            # (1) Compute image-space gradient dL/dx (via autograd, without relying on full BP chain)\r\n            # Ensure imgs retains gradients\r\n            imgs.retain_grad()\r\n            g_x = torch.autograd.grad(loss, imgs, retain_graph=True, create_graph=False)[0]\r\n\r\n            g_x_denoised = fourier_denoise_tensor(g_x, cutoff=0.05)\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=g_x_denoised, retain_graph=False)[0]\r\n\r\n            w_batch.grad = w_grad\r\n\r\n            optimizer.step()\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # Logging / visualization\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and ((i + 1) % 10 == 0 or i == 0)\r\n                if vis_condition:\r\n                    torchvision.utils.save_image(\r\n                        torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                        os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png')\r\n                    )\r\n\r\n                    grad_x_vis = g_x.detach().cpu()\r\n                    # Normalize gradient for better visualization\r\n                    grad_x_vis = (grad_x_vis - grad_x_vis.min()) / (grad_x_vis.max() - grad_x_vis.min() + 1e-8)\r\n                    save_grid_with_cmap(\r\n                        grad_x_vis,\r\n                        os.path.join(self.save_dir,\r\n                                     f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    grad_x_vis = g_x_denoised.detach().cpu()\r\n                    grad_x_vis = (grad_x_vis - grad_x_vis.min()) / (grad_x_vis.max() - grad_x_vis.min() + 1e-8)\r\n                    save_grid_with_cmap(\r\n                        grad_x_vis,\r\n                        os.path.join(self.save_dir,\r\n                                     f'grad_x_PCA_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def optimize(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n        self.visualize_intermediate_results = 0\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0)\r\n\r\n            # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            imgs.retain_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    torchvision.utils.save_image(\r\n                        torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                        os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png')\r\n                    )\r\n\r\n                    grad_x = imgs.grad.detach().cpu()\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def optimize_with_adv_model(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = self.compute_discriminator_logit(imgs)\r\n\r\n                # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.adv_model(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            imgs.retain_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    torchvision.utils.save_image(\r\n                        torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                        os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png')\r\n                    )\r\n\r\n                    grad_x = imgs.grad.detach().cpu()\r\n                    grad_x = (grad_x - grad_x.min()) / (grad_x.max() - grad_x.min() + 1e-8)\r\n                    save_grid_with_cmap(\r\n                        grad_x, os.path.join(self.save_dir,\r\n                                             f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def smooth_grad_optimization(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        # Initialize attack\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        self.visualize_intermediate_results = 1\r\n\r\n        num_samples = 30  # SmoothGrad sample count\r\n        # sigma = 0.1           # SmoothGrad standard deviation\r\n        stdev_spread = 0.05\r\n\r\n        # Start optimization\r\n        for i in range(num_epochs):\r\n            # synthesize imagesnd preprocess images\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            # compute discriminator loss\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = self.compute_discriminator_logit(imgs)\r\n\r\n            # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            # Compute target loss\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            # combine losses and compute gradients\r\n            optimizer.zero_grad()\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n            sigma = stdev_spread * (imgs.max().item() - imgs.min().item())\r\n            # sigma = stdev_spread * (imgs.amax(dim=(1, 2, 3)) - imgs.amin(dim=(1, 2, 3)))\r\n            # sigma = sigma.view(-1, 1, 1, 1)\r\n            # print(sigma)\r\n\r\n            # Use SmoothGrad to compute smoothed gradients:\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                noisy_imgs = imgs + torch.randn_like(imgs) * sigma\r\n                outputs_noise = self.target(noisy_imgs)\r\n                loss_noise = poincare_loss(outputs_noise, targets_batch).mean()\r\n\r\n                grad_noise = torch.autograd.grad(loss_noise, imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_noise\r\n\r\n            smooth_grad /= num_samples\r\n            smooth_grad_x = smooth_grad.detach().cpu()\r\n\r\n            w_grad = torch.autograd.grad(outputs=imgs, inputs=w_batch, grad_outputs=smooth_grad, retain_graph=False)[0]\r\n            w_batch.grad = w_grad\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    # torchvision.utils.save_image(\r\n                    #     torchvision.utils.make_grid(imgs.detach().cpu(), nrow=5, normalize=True, scale_each=True),\r\n                    #     os.path.join(self.save_dir, f'Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png')\r\n                    # )\r\n\r\n                    save_grid_with_cmap(\r\n                        grad_x.detach().cpu(), os.path.join(self.save_dir,\r\n                                                            f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def smooth_grad_optimization_with_transforms(self, batch_i, w_batch, targets_batch, num_epochs):\r\n        optimizer = self.config.create_optimizer(params=[w_batch.requires_grad_()])\r\n        scheduler = self.config.create_lr_scheduler(optimizer)\r\n\r\n        num_samples = 30  # 50\r\n        transformations = build_transformations()\r\n\r\n        for i in range(num_epochs):\r\n            imgs = self.synthesize(w_batch, num_ws=self.num_ws)\r\n\r\n            if self.discriminator_weight > 0:\r\n                discriminator_loss = self.compute_discriminator_loss(imgs)\r\n            else:\r\n                discriminator_loss = torch.tensor(0.0)\r\n\r\n            # perform image transformations\r\n            if self.clip:\r\n                imgs = self.clip_images(imgs)\r\n            if self.transformations:\r\n                imgs = self.transformations(imgs)\r\n\r\n            outputs = self.target(imgs)\r\n            target_loss = poincare_loss(outputs, targets_batch).mean()\r\n\r\n            loss = target_loss + discriminator_loss * self.discriminator_weight\r\n\r\n            optimizer.zero_grad()\r\n\r\n            grad_x = torch.autograd.grad(loss, imgs, retain_graph=True)[0]\r\n            grad_x = grad_x.detach().cpu()\r\n\r\n            smooth_grad = 0\r\n            for j in range(num_samples):\r\n                trans_imgs = transformations(imgs)  # here, transformations is T.Compose(...)\r\n\r\n                outputs_noise = self.target(trans_imgs)\r\n                loss_noise = poincare_loss(outputs_noise, targets_batch).mean()\r\n\r\n                grad_noise = torch.autograd.grad(loss_noise, imgs, retain_graph=True)[0]\r\n                smooth_grad += grad_noise\r\n\r\n            smooth_grad /= num_samples\r\n            smooth_grad_x = smooth_grad.detach().cpu()  # for visualization\r\n\r\n            # 9) Project smoothed gradient back to w_batch\r\n            #    dL/dw = J_G(w)^T * smooth_grad\r\n            w_grad = torch.autograd.grad(\r\n                outputs=imgs,\r\n                inputs=w_batch,\r\n                grad_outputs=smooth_grad,\r\n                retain_graph=False)[0]\r\n\r\n            w_batch.grad = w_grad\r\n            optimizer.step()\r\n\r\n            if scheduler:\r\n                scheduler.step()\r\n\r\n            # Log results\r\n            if self.log_progress or self.visualize_intermediate_results:\r\n                with torch.no_grad():\r\n                    confidence_vector = outputs.softmax(dim=1)\r\n                    confidences = torch.gather(confidence_vector, 1, targets_batch.unsqueeze(1))\r\n                    mean_conf = confidences.mean().detach().cpu()\r\n                    min_conf = confidences.min().detach().cpu()\r\n                    max_conf = confidences.max().detach().cpu()\r\n\r\n            # Log results\r\n            if self.log_progress:\r\n                log_condition = (i + 1) % 10 == 0 or i == 0\r\n                if log_condition:\r\n                    print(\r\n                        f'iteration {i + 1}: \\t total_loss={loss:.4f} \\t target_loss={target_loss:.4f} \\t',\r\n                        f'discriminator_loss={discriminator_loss:.4f} \\t mean_conf={mean_conf:.4f} ({min_conf:.4f}, {max_conf:.4f})'\r\n                    )\r\n\r\n            if self.visualize_intermediate_results:\r\n                vis_condition = self.save_dir and (i + 1) % 10 == 0 or i == 0\r\n                if vis_condition:\r\n                    save_grid_with_cmap(\r\n                        grad_x.detach().cpu(), os.path.join(self.save_dir,\r\n                                                            f'grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n                    save_grid_with_cmap(\r\n                        smooth_grad_x, os.path.join(self.save_dir,\r\n                                                    f'smooth_grad_x_Batch_{batch_i + 1}_iter_{i + 1:02d}_Acc_{mean_conf:.4f}.png'),\r\n                        nrow=5\r\n                    )\r\n\r\n            torch.cuda.empty_cache()\r\n\r\n        return w_batch.detach()\r\n\r\n    def compute_proj_x_orthogonalized_2(self, imgs, w_batch):\r\n        \"\"\"\r\n        Compute the orthogonal projection of the real gradient (imgs.grad) onto the column space\r\n        of the generator Jacobian (i.e., the manifold tangent space).\r\n\r\n        Math:\r\n            1) J = dG/dz, computed via torch.autograd.functional.jacobian\r\n            2) Perform SVD on J to get U\r\n            3) proj_x = U @ (U.T @ real_grad)\r\n\r\n        Args:\r\n        - imgs: image tensor that contains generator outputs and has retain_grad() called,\r\n                shape [B, C, H, W]; imgs.grad stores the real gradient\r\n        - w_batch: [B, 1, latent_dim], a batch of latent variables\r\n\r\n        Returns:\r\n        - proj_x_batch: [B, C, H, W], projected gradient\r\n        \"\"\"\r\n        from functorch import vmap, jvp\r\n\r\n        proj_x_list = []\r\n        B, _, latent_dim = w_batch.shape\r\n        device = w_batch.device\r\n        eye = torch.eye(latent_dim, device=device)\r\n\r\n        # def G_forward(w):\r\n        #     gen = self.synthesis.module if hasattr(self.synthesis, 'module') else self.synthesis\r\n        #     return gen(w)  # here we do not pass num_ws\r\n\r\n        def G_forward(w):\r\n            gen = self.synthesis.module if hasattr(self.synthesis, 'module') else self.synthesis\r\n            return gen(w)\r\n\r\n        for i in range(B):\r\n            # Take the i-th sample w_i with shape [1, latent_dim] and expand to [1, num_ws, latent_dim]\r\n            w_i = w_batch[i].detach().requires_grad_()  # shape: [1, latent_dim]\r\n            w_i = w_i.unsqueeze(0).repeat(1, self.num_ws, 1)  # [1, num_ws, latent_dim]\r\n\r\n            output_shape = imgs.grad[i].shape\r\n\r\n            def flattened_g(_w):\r\n                out = G_forward(_w)\r\n\r\n                if self.transformations:\r\n                    out = self.transformations(out)\r\n\r\n                return out.view(-1)\r\n\r\n            # Use vmap to compute JVP for all latent directions in one pass\r\n            # jvp_all = vmap(lambda v: jvp(flattened_g, (w_i,), (v.unsqueeze(0).unsqueeze(0).expand(1, self.num_ws, -1),))[1])(eye)\r\n            jvp_all = vmap(\r\n                lambda v: jvp(flattened_g, (w_i,), (v.unsqueeze(0).unsqueeze(0).expand(1, self.num_ws, -1),))[1],\r\n                randomness=\"same\"\r\n            )(eye)\r\n\r\n            # Transpose to get the J matrix with shape [output_dim, latent_dim]\r\n            J = jvp_all.transpose(0, 1)\r\n\r\n            # Perform SVD to obtain U\r\n            U, _, _ = torch.linalg.svd(J, full_matrices=False)\r\n\r\n            # Extract the real image gradient (assume imgs.grad exists) and flatten to [output_dim]\r\n            real_grad = imgs.grad[i].view(-1)\r\n\r\n            proj_x = U @ (U.T @ real_grad)\r\n            proj_x = proj_x.view(output_shape)\r\n            proj_x_list.append(proj_x.detach())\r\n\r\n            norm_real = torch.norm(real_grad)\r\n            norm_proj = torch.norm(proj_x)\r\n            ratio = norm_proj / norm_real if norm_real > 0 else torch.tensor(0.0)\r\n            print(f\"Sample {i}: Projection ratio = {ratio.item():.6f}\")\r\n\r\n        return torch.stack(proj_x_list)\r\n\r\n    def synthesize(self, w, num_ws):\r\n        if w.shape[1] == 1:\r\n            w_expanded = torch.repeat_interleave(w,\r\n                                                 repeats=num_ws,\r\n                                                 dim=1)\r\n            imgs = self.synthesis(w_expanded,\r\n                                  noise_mode='const',\r\n                                  force_fp32=True)\r\n        else:\r\n            imgs = self.synthesis(w, noise_mode='const', force_fp32=True)\r\n        return imgs\r\n\r\n    def clip_images(self, imgs):\r\n        lower_limit = torch.tensor(-1.0).float().to(imgs.device)\r\n        upper_limit = torch.tensor(1.0).float().to(imgs.device)\r\n        imgs = torch.where(imgs > upper_limit, upper_limit, imgs)\r\n        imgs = torch.where(imgs < lower_limit, lower_limit, imgs)\r\n        return imgs\r\n\r\n    def compute_discriminator_loss(self, imgs):\r\n        discriminator_logits = self.discriminator(imgs, None)\r\n        discriminator_loss = nn.functional.softplus(\r\n            -discriminator_logits).mean()\r\n        return discriminator_loss\r\n\r\n    def compute_discriminator_logit(self, imgs):\r\n        discriminator_logits = self.discriminator(imgs, None).mean()\r\n        return discriminator_logits\r\n\r\n# '''\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/high_resolution/attacks/optimize.py b/high_resolution/attacks/optimize.py
--- a/high_resolution/attacks/optimize.py	
+++ b/high_resolution/attacks/optimize.py	
@@ -25,37 +25,21 @@
 
 def build_transformations(transformations_dict=None) -> T.Compose:
     transformations_dict = {
-        # "RandomResizedCrop": {
-        #     "size": 224,
-        #     "scale": [0.8, 1.0],
-        #     "ratio": [0.9, 1.1],
-        #     "antialias": True
-        # },
-        # "RandomHorizontalFlip": {
-        #     "p": 0.5
-        # },
-        # "RandomRotation": {
-        #     "degrees": 5,  # ±15°
-        #     "interpolation": InterpolationMode.BILINEAR,
-        #     "expand": False,
-        #     "center": None
-        # },
-
         "RandomResizedCrop": {
             "size": 224,
-            "scale": [0.7, 1.0],
-            "ratio": [0.8, 1.2],
+            "scale": [0.8, 1.0],
+            "ratio": [0.9, 1.1],
             "antialias": True
         },
         "RandomHorizontalFlip": {
             "p": 0.5
         },
         "RandomRotation": {
-            "degrees": 5,  # ±15°
+            "degrees": 5,  # ±5°
             "interpolation": InterpolationMode.BILINEAR,
             "expand": False,
             "center": None
-        }
+        },
     }
 
     transformation_list = []
